---
title: "Analyse des données de consommations d'énergie en Inde"
output: html_notebook
---

# Environnement de travail

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(ggplot2)
library(readxl)
library(tidyr)
library(fda)
```

# Notre base de données

Notre étude porte sur l'analyse de la consommation d'énergie dans les 33 Etats d'Inde. Il est à noter que l’Inde est le troisième producteur et le troisième consommateur d’électricité au monde.


## Importation des données

Dans un premier temps, nous allons importer les données.

```{r}
data <- read.csv("long_data_2.csv", stringsAsFactors = T,sep = ";")

colnames(data) <- c("Etat", "Regions", "latitude", "longitude", "Date", "Consommation")
data <- data[,-c(3,4)]
summary(data)
# View(data)
```

Grâce à ce résumé statistique, nous pouvons voir que les valeurs de nos données sont assez étendues.\

```{r}
data$Etat[which.max(data$Consommation)]
data$Etat[which.min(data$Consommation)]
```

En effet, certaines valeurs journalières sont très faibles (inférieures à 1) alors que d'autres sont 500 fois plus élevées. Par exemple, le Maharashtra qui présente certaines valeurs très importantes en opposition avec le Sikkim. 

Concernant notre base de données, elle décrit la consommation d'énergie dans **33 Etats** d'Inde entre 2019 et 2020. \
Nous avons 16 599 lignes, soit **503 points de mesure** par pays (503*33). Les données sont journalières, exprimées en Mega Units.

```{r}
dim(data)
```


## Choix de la période étudiée

Nous allons transformer notre variable Date au format date pour poursuivre l'étude correctement.

```{r}
data$Date <- as.Date(data$Date, format = "%d/%m/%Y %H:%M")
str(data) # Vérification
```

Pour simplifier notre analyse, nous allons nous concentrer seulement sur l'année 2019 : 

```{r}
data <- data |> 
  filter(Date <= as.Date("2020-01-02"))
```

On vérifie à présent que les données que l'on va analyser correspondent bien à la période choisie. 

```{r}
min(data$Date)
max(data$Date)
```

## Normalisation des données 

Au départ, nous avons voulu normaliser les données par le nombre d'habitants de chaque Etat pour que les consommations aient le même impact dans notre étude. Cependant, cette méthode s'est révélée problématique car nous avons obtenu des valeurs très proches de zéro, ce qui aurait compliqué les calculs.

Nous avons donc choisi de réduire les données de consommation, en soustrayant la moyenne, pour continuer notre analyse avec données moins disproportionnées.

```{r}
moyenne_par_etat <- tapply(data$Consommation, data$Etat, mean)

normalized_energy <- data
for(i in 1:nrow(normalized_energy)){
  normalized_energy$Consommation[i] <- normalized_energy$Consommation[i] - moyenne_par_etat[normalized_energy$Etat[i]]
}
```


Suite à cette normailisation, nous pouvons observer une diminution importante des valeurs. Mais nous obtenons des valeurs de consommations négatives, ce qui pose problème pour l'interprétation de nos analyses. 
Nous allons donc travailler avec les valeurs initiales. 

## Représentations graphiques 

Nous allons tout d'abord représenter simplement nos données pour pouvoir faire ressortir les informations importantes. 

```{r}
# Utilisation de la fonction gather pour mettre en forme les données
# data_long <- tidyr::gather(normalized_energy, key = "Etat", value = "Consommation", -Date)

# Création du graphique avec ggplot
ggplot(data, aes(x = Date, y = Consommation, color = Etat)) +
  geom_line() +
  labs(x = "Date", y = "Consommation d'énergie", title = "Consommation d'énergie par état") +
  theme_minimal()
```
Ce graphique nous montre que nos données sont très irrégulières. \

[Représentation des données en deux parties].{underline}

Pour pouvoir que les informations soient plus lisibles, nous avons décidé de séparer en plusieurs parties nos données. \
Nous avons fixé des seuils par rapport à la moyenne de consommations des Etats pour réaliser ces graphiques. 


On calcule d'abord la moyenne par Etat et on ajoute cette moyenne a un nouveau dataframe : 

```{r}
moyenne_par_etat <- tapply(data$Consommation, data$Etat, mean)
moyenne_par_etat <- data.frame(Etat = names(moyenne_par_etat), Moyenne = moyenne_par_etat)

data_complete <- merge(data,moyenne_par_etat, by = "Etat") 
```

On sépare les données en 3 catégories : 

```{r}
data_inf <- data_complete |> 
  filter(Moyenne <= 50)
# data_inf

data_moy <- data_complete |> 
  filter(Moyenne > 50 & Moyenne < 200)
# data_moy

data_sup <- data_complete |> 
  filter(Moyenne >= 200)
# data_sup
```

```{r}
ggplot(data_inf, aes(x = Date, y = Consommation, color = Etat)) +
  geom_line() +
  labs(x = "Date", y = "Consommation d'énergie", title = "Consommation d'énergie par état", subtitle = "Etats consommant moins de 50 Mega Units en moyenne") +
  theme_minimal()

ggplot(data_moy, aes(x = Date, y = Consommation, color = Etat)) +
  geom_line() +
  labs(x = "Date", y = "Consommation d'énergie", title = "Consommation d'énergie par état", subtitle = "Etats consommant entre 50 et 200 Mega Units en moyenne") +
  theme_minimal()

ggplot(data_sup, aes(x = Date, y = Consommation, color = Etat)) +
  geom_line() +
  labs(x = "Date", y = "Consommation d'énergie", title = "Consommation d'énergie par état", subtitle = "Etats consommant plus de 200 Mega Units en moyenne") +
  theme_minimal()
```

Grâce à ces représentations, il est plus facile d'observer et d'étudier l'allure de nos données.\

Pour la plupart des Etats, leur consommation varie beaucoup tout au long de l'année et en quelques jours leur consommation peut doublé ou bien diminuer subitement.

Cependant certains Etats comme le Tripura possède une consommation très faible et peu de variation dans son signal est observé :

```{r}
data |> 
  filter(Etat=="Tripura") |> 
  ggplot(aes(x = Date, y = Consommation)) +
  geom_line() +
  labs(x = "Date", y = "Consommation d'énergie", title = "Consommation d'énergie par état", subtitle = "Etats consommant moins de 50 Mega Units en moyenne") +
  theme_minimal()
```

Sa consommation varie entre 2.4 et 6.6 mega Units.

```{r}
min(data$Consommation[data$Etat=="Tripura"])
max(data$Consommation[data$Etat=="Tripura"])
```

A l'opposition du Punjab qui lui possède une très grande variation dans sa consommation :

```{r}
min(data$Consommation[data$Etat=="Punjab"])
max(data$Consommation[data$Etat=="Punjab"])
```

[Tableau des moyennes de consommation par Etat]{.underline}

```{r}
tapply(data$Consommation, data$Etat, mean)
```



[Représentation selon la région]{.underline}

L'inde est divisé en 5 regions : le Nord (NR), Nord-Est (NER), le Sud (SR), l'Est (ER) et l'Ouest (WR)
Nous traçons les consommations en fonction des régions (une couleur par région)./
De la même manière que les représentations précédentes, nous ne pouvons représenter les régions sur le même graphique. Sans diviser la représentation, il est difficile de bien observer les différences entre les régions.

En reprenant la même méthode que pour les Etats, nous calculons les moyennes des consommations par Regions:

```{r}
moyenne_par_reg <- tapply(data$Consommation, data$Regions, mean)
moyenne_par_reg <- data.frame(Regions = names(moyenne_par_reg), Moyenne_reg = moyenne_par_reg)

data_complete_reg <- merge(data_complete,moyenne_par_reg, by = "Regions")
```

```{r}
data_inf <- data_complete_reg |> 
  filter(Moyenne_reg <= 66)
# data_inf

data_moy <- data_complete_reg |> 
  filter(Moyenne_reg > 66 & Moyenne_reg < 150)
# data_moy

data_sup <- data_complete_reg |> 
  filter(Moyenne_reg >= 150)
# data_sup
```

```{r}
ggplot(data_inf, aes(x = Date, y = Consommation, color = Regions)) +
  geom_line() +
  labs(x = "Date", y = "Consommation d'énergie", title = "Consommation d'énergie par état", subtitle = "Pays consommant moins de 50 Mega Units en moyenne") +
  theme_minimal()

ggplot(data_moy, aes(x = Date, y = Consommation, color = Regions)) +
  geom_line() +
  labs(x = "Date", y = "Consommation d'énergie", title = "Consommation d'énergie par état", subtitle = "Pays consommant entre 50 et 200 Mega Units en moyenne") +
  theme_minimal()

ggplot(data_sup, aes(x = Date, y = Consommation, color = Regions)) +
  geom_line() +
  labs(x = "Date", y = "Consommation d'énergie", title = "Consommation d'énergie par état", subtitle = "Pays consommant plus de 200 Mega Units en moyenne") +
  theme_minimal()
```

Cependant, ces représentations graphiques ne nous permettent pas de bien visualiser et n'apportent pas d'informations.\

Ainsi, nous décidons donc de représenter une région par graphique :

```{r}
data_complete_reg |> 
  filter(Regions=="ER") |> 
  ggplot(aes(x = Date, y = Consommation,color = Etat)) +
  geom_line() +
  labs(x = "Date", y = "Consommation d'énergie", title = "Consommation d'énergie pour les Etats de l'Est de l'Inde") +
  theme_minimal()

data_complete_reg |> 
  filter(Regions=="NER") |> 
  ggplot(aes(x = Date, y = Consommation,color = Etat)) +
  geom_line() +
  labs(x = "Date", y = "Consommation d'énergie", title = "Consommation d'énergie pour les Etats du Nord Est de l'Inde") +
  theme_minimal()

data_complete_reg |> 
  filter(Regions=="NR") |> 
  ggplot(aes(x = Date, y = Consommation,color = Etat)) +
  geom_line() +
  labs(x = "Date", y = "Consommation d'énergie", title = "Consommation d'énergie pour les Etats du Nord de l'Inde") +
  theme_minimal()

data_complete_reg |> 
  filter(Regions=="SR") |> 
  ggplot(aes(x = Date, y = Consommation,color = Etat)) +
  geom_line() +
  labs(x = "Date", y = "Consommation d'énergie", title = "Consommation d'énergie pour les Etats du Sud de l'inde") +
  theme_minimal()

data_complete_reg |> 
  filter(Regions=="WR") |> 
  ggplot(aes(x = Date, y = Consommation,color = Etat)) +
  geom_line() +
  labs(x = "Date", y = "Consommation d'énergie", title = "Consommation d'énergie pour les Etats de l'Ouest de l'Inde") +
  theme_minimal()

```

Au sein de chaque région de l'Inde, nous observons de fortes différences de consommation. Par exemple, dans le Sud de l'Inde, Pondy a une faible consommation d'énergie (entre 4 et 10 mega Units) à l'opposé de Tamil Nadu où sa consommation varie entre 200 et 370 mega Units.

Il serait intéressant de consulter l'avis d'un économiste ou politicien pour analyser et comprendre ces grandes disparités au sein des régions de l'Inde.


Nous construisons le tableau des moyennes de consommation par région :

```{r}
tapply(data$Consommation, data$Regions, mean)
```

Nous observons une grande disparité entre les régions de l'Inde. En effet, le Nord-Est à une consommation très faible par rapport à l'Ouest.



# Ondelettes


Puisque nos signaux sont irréguliers, notre choix de lissage c'est tourné vers les ondelettes car elles ont des bonnes propriétés de compressions et sont adaptées à ce type de données. 

Le problème ici est que la longueur des courbes de consommation n'est pas de la forme $2^J$. Pour la courbe de PH de longueur 356, on symétrise le signal à la fin pour atteindre une longueur de 512. 
On se ramènera à la fin au signal de départ de taille 356.

```{r, message=FALSE, warning=FALSE}
data_long <- data.frame("Etat" = data$Etat,"Date"= data$Date,"Consommation" = data$Consommation)

data_long <- data_long |> 
  pivot_wider(names_from = "Etat", values_from = "Consommation")
```

Nous représentons le signal pour un Etat (ici le Punjab) :

```{r}
i = 2
y <- as.vector(t(unlist(data_long[, i])))

plot(1:356,y,type="l",main="Signal de départ", ylab = "Consommation", xlab="Jour")

# Créer un signal symétrisé
ysym = c(y, rev(tail(y, n= 512-length(y)))) # on choisit 512 pour avoir une puissance de 2

# Tracer le signal symétrisé
plot(ysym, type = "l", main = "Signal symétrisé", ylab = "Consommation", xlab = "Jour")

# Ajouter une ligne verticale à la position 
# axe de symétrie : 361 jours 
abline(v = 356, lty = 2)
```



# Lissage par moindres carrés pénalisés

## Lissage d'une trajectoire

```{r}
y <- as.vector(t(unlist(data_long[, i]))) # courbe du premier Etat : Punjab
tps <- 1:356
plot(tps,y,type="l", main = "Consommation d'énergie du Punjab")
```

1. Lissage par moindres carrés pénalisés sur la première courbe


```{r,message=FALSE,warning=FALSE}
splbasis = create.bspline.basis(rangeval = c(1,356),
                                norder=4, 
                                breaks=c(1:25))
# définition une base de B-spline entre 1 et 356 (les dates) 
# ordre 4 (classique) 
# breaks : coupure

# On fait l'ajustement pour différentes valeurs de lambda pour minimiser le critère de pénalité
# On veut la fonction la plus proche des points et la moins oscillante grâce à la pénalité

gcv = rep(0,30) # grille de lambda

for (i in 1:30){
  
  lambda = exp(i-10)
  
  fdpar = fdPar(splbasis,Lfdobj = 2,lambda=lambda) # calcul de la valeur de pénalité
  # Lfdoj = 2 : ordre de la dérivée dans la pénalité (dérivée seconde)
  
  smoothdata = smooth.basis(tps,y, fdParobj = fdpar) # ajustement de l'objet fonctionnel
  
  # smooth.basis : équivalent à Data2fd mais AVEC pénalité
  # donne la valeur du critère VC de Moindres Carrés Généralisé
  
  gcv[i] = smoothdata$gcv # gcv : general cross validation
}
plot(gcv)
which.min(gcv)
```

On voit que le minimum est la 13ème valeur 

Nous allons recalculer la pénalité et refaire le lissage avec la valeur de lambda minimale : 

```{r}
lambda = exp(which.min(gcv)-10)
fdpar = fdPar(splbasis,Lfdobj = 2,lambda=lambda) # calcul pénalité
smoothdata = smooth.basis(tps,y,fdParobj = fdpar) # lissage
```

Nous représentons sur un même graphe les observations et la fonction estimée reconstruite ˆf.

```{r,message=FALSE,warning=FALSE}
plotfit.fd(y,# valeur de PH pour un pays
           tps, # les dates
           smoothdata$fd, # nos données estimées
           # 1 à 356 
           pch=20,
           cex=0.5,
           main="les observations et la fonction estimée reconstruite ˆf",
           xlab = "PH",
           ylab="temps")
```
On a représenté les points et l'objet fonctionnel. 

# IDEE :
la prof a dit que ça dépendait de nos objectifs. Comme on a pas une base d'ondelettes, est-ce qu'on va réellement pouvoir reproduire le signal ?
Donc notre objectif ça pourrait être d'avoir un signal "+ lisse" ?
à voir

```{r}
fhatprim = eval.fd(tps,smoothdata$fd,Lfdobj=1) # dérivé première
# eval.fd : évalue sur les dates
fhatpprim = eval.fd(tps,smoothdata$fd,Lfdobj=2) # dérivé seconde
matplot(tps,cbind(fhatprim,fhatpprim),type="l")
```

La dérivée première est en noir et la dérivée seconde en rouge.


### Lissage de l'ensemble des trajectoires

Nous allons lisser une par une les signaux de chaque Etats.

# PROBLEME ICI A REVOIR 

```{r}
splbasis = create.bspline.basis(rangeval = c(1,356),
                                norder=4, 
                                breaks=c(1:25))

gcv = matrix(0, nrow = 30, ncol = 34) # 30 valeur pour chacun des 33 etats
for(e in 2:34){ # Pour chacun des Etats
  for (i in 1:30){
    y <- as.vector(t(unlist(data_long[, e])))
    lambda = exp(i-10)
    fdparTemp = fdPar(splbasis,Lfdobj = 2,lambda=lambda)
    smoothdata = smooth.basis(tps,y,fdParobj = fdparTemp) # Plus un seul individu mais plusieurs individu
    gcv[i,e] = mean(smoothdata$gcv) 
  # avant un critère de VC pour chaque individu
  # maintenant comme n courbes donc n valeur de critère pour un lambda donné
  # donc moyenne des critères de VC
  }
}
plot(gcv)
which.min(gcv)
```

```{r}
lambda = exp(which.min(gcv)-10)
fdparTemp = fdPar(splbasis,Lfdobj = 2,lambda=lambda)
smoothdata = smooth.basis(age,growth$hgtm,fdParobj = fdparTemp)
```

plotfit.fd --\> un graphique par individu (moins bien)

[Représentation des données brutes et des données lisses]{.underline}

```{r}
fhatsmooth = eval.fd(age,smoothdata$fd) # on évalue notre objet fonctionnel sur nos âge

par(mfrow=c(1,2))
matplot(tps,growth$hgtm,type="p",lty=1,ylab="",main="donnees brutes")
matplot(tps,fhatsmooth,type="p",lty=1,ylab="",main="donnees lissees")
```

Très proche

la différence que l'on voit pas est ?

[Représentation des résidus]{.underline}

```{r}
matplot(age,fhatsmooth-growth$hgtm,lty=1,type="l",ylab="residus")
```




# A FAIRE

base de spline pénalisé



# Sources 

- base de données : https://www.kaggle.com/datasets/twinkle0705/state-wise-power-consumption-in-india?fbclid=IwAR3tNVrXpTrRlO_HVOoa53bcaTR8aR7QU_Nlbe4AhhmfzdK9ERa5Xxid1yk
- nombre d'habitants par pays : https://fr.zhujiworld.com/in/673630-tripura/?fbclid=IwAR2Ua_lx2k7u-Nil065Pw0S0mK682X7e8AuvR5CS7lfpsbUBPIlEzNiwE2Q